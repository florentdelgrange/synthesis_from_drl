{"self": "<wasserstein_mdp.WassersteinMarkovDecisionProcess object at 0x14e40010fdf0>", "action_shape": "(6,)", "reward_shape": "(1,)", "label_shape": "(2,)", "discretize_action_space": "False", "state_encoder_network": "ModelArchitecture(hidden_units=[256, 256], activation='dsilu', output_dim=None, input_dim=None, name=None, batch_norm=False, filters=None, kernel_size=None, strides=None, padding=None, raw_last=False, transpose=False, max_pooling=False, avg_pooling=False)", "action_decoder_network": "None", "transition_network": "ModelArchitecture(hidden_units=[256, 256], activation='dsilu', output_dim=None, input_dim=None, name=None, batch_norm=False, filters=None, kernel_size=None, strides=None, padding=None, raw_last=True, transpose=False, max_pooling=False, avg_pooling=False)", "reward_network": "ModelArchitecture(hidden_units=[256, 256], activation='dsilu', output_dim=None, input_dim=None, name=None, batch_norm=False, filters=None, kernel_size=None, strides=None, padding=None, raw_last=True, transpose=False, max_pooling=False, avg_pooling=False)", "decoder_network": "ModelArchitecture(hidden_units=[256, 256], activation='dsilu', output_dim=None, input_dim=None, name=None, batch_norm=False, filters=None, kernel_size=None, strides=None, padding=None, raw_last=True, transpose=False, max_pooling=False, avg_pooling=False)", "latent_policy_network": "None", "steady_state_lipschitz_network": "ModelArchitecture(hidden_units=[256, 256], activation='dsilu', output_dim=None, input_dim=None, name=None, batch_norm=False, filters=None, kernel_size=None, strides=None, padding=None, raw_last=True, transpose=False, max_pooling=False, avg_pooling=False)", "transition_loss_lipschitz_network": "ModelArchitecture(hidden_units=[256, 256], activation='dsilu', output_dim=None, input_dim=None, name=None, batch_norm=False, filters=None, kernel_size=None, strides=None, padding=None, raw_last=True, transpose=False, max_pooling=False, avg_pooling=False)", "latent_state_size": "14", "number_of_discrete_actions": "None", "action_encoder_network": "None", "state_encoder_pre_processing_network": "[None, None, ModelArchitecture(hidden_units=None, activation='relu', output_dim=None, input_dim=None, name=None, batch_norm=False, filters=[32, 32], kernel_size=[7, 4], strides=[2, 2], padding=['valid', 'valid'], raw_last=True, transpose=False, max_pooling=False, avg_pooling=False)]", "state_decoder_post_processing_network": "None", "time_stacked_states": "False", "state_encoder_temperature": "0.999", "state_prior_temperature": "0.5", "action_encoder_temperature": "None", "latent_policy_temperature": "None", "wasserstein_regularizer_scale_factor": "WassersteinRegularizerScaleFactor(global_scaling=None, global_gradient_penalty_multiplier=10, steady_state_scaling=0.01, steady_state_gradient_penalty_multiplier=None, local_transition_loss_scaling=0.1, local_transition_loss_gradient_penalty_multiplier=None)", "encoder_temperature_decay_rate": "0.0", "prior_temperature_decay_rate": "0.0", "reset_state_label": "True", "minimizer": "<keras.optimizer_v2.adam.Adam object at 0x14e40010fd30>", "maximizer": "<keras.optimizer_v2.adam.Adam object at 0x14e40010fa30>", "encoder_optimizer": "<keras.optimizer_v2.adam.Adam object at 0x14e40010fd30>", "entropy_regularizer_scale_factor": "0.0", "entropy_regularizer_decay_rate": "0.0", "entropy_regularizer_scale_factor_min_value": "0.0", "importance_sampling_exponent": "1.0", "importance_sampling_exponent_growth_rate": "0.0", "time_stacked_lstm_units": "128", "reward_bounds": "None", "latent_stationary_network": "None", "action_entropy_regularizer_scaling": "0.0", "enforce_upper_bound": "False", "squared_wasserstein": "False", "n_critic": "3", "trainable_prior": "False", "state_encoder_type": "EncodingType.DETERMINISTIC", "policy_based_decoding": "False", "deterministic_state_embedding": "True", "state_encoder_softclipping": "False", "steady_state_net_softclipping": "True", "external_latent_policy": "<tf_agents.policies.greedy_policy.GreedyPolicy object at 0x14e40010ea40>", "input_name": "('game_variables', 'label', 'screen')", "input_state_component_concat_units": "64", "cost_fn": "{'state': ('l22', 'l22', 'l22'), 'reward': 'l2'}", "cost_weights": "{'state': (0, 0, 0), 'reward': 0.1}", "use_batch_norm": "False", "summary": "True", "clip_by_global_norm": "5", "use_total_variation": "True", "softclip_fn": "dsilu", "straight_through": "False", "args": "()", "kwargs": "{}", "state_shape": "[(7,), (2,), (45, 60, 3)]", "__class__": "<class 'wasserstein_mdp.WassersteinMarkovDecisionProcess'>", "eval_policy": "-93.3681183", "training_step": "133334"}